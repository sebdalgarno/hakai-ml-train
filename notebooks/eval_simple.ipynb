{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Model Evaluation\n",
    "\n",
    "Load one model, compute IoU on validation set. Should match W&B `val/iou_epoch/seagrass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import torch\n",
    "import yaml\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torchmetrics\n",
    "\n",
    "from src.data import NpzSegmentationDataset\n",
    "from src.models.smp import SMPMulticlassSegmentationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG - edit these paths\n",
    "CONFIG_PATH = \"../configs/seagrass-rgb/architecture-experiment/segformer_mitb2_1024.yaml\"\n",
    "CKPT_PATH = \"/mnt/class_data/sdalgarno/checkpoints/architecture-experiment/segformer-1024/last.ckpt\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load config\nwith open(CONFIG_PATH) as f:\n    config = yaml.safe_load(f)\n\nmodel_args = config[\"model\"][\"init_args\"]\ndata_args = config[\"data\"][\"init_args\"]\n\nprint(f\"Model: {model_args['architecture']} / {model_args['backbone']}\")\nprint(f\"Val dir: {data_args['val_chip_dir']}\")\n\n# IMPORTANT: Verify this matches what W&B shows for this run!\n# Check W&B -> Run -> Config -> data.init_args.val_chip_dir"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load validation dataset\ntest_transforms = A.from_dict(data_args[\"test_transforms\"])\nval_dataset = NpzSegmentationDataset(data_args[\"val_chip_dir\"], transforms=test_transforms)\nval_loader = DataLoader(val_dataset, batch_size=8, num_workers=4, shuffle=False)\n\nprint(f\"Validation tiles: {len(val_dataset)}\")\nprint(f\"\\nFirst 5 files:\")\nfor f in list(Path(data_args[\"val_chip_dir\"]).glob(\"*.npz\"))[:5]:\n    print(f\"  {f.name}\")\n\n# VERIFY: Does this tile count match W&B validation batches?\n# W&B batches * batch_size â‰ˆ num tiles"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run evaluation - EXACTLY matching training code\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in tqdm(val_loader, desc=\"Evaluating\"):\n        images = images.to(DEVICE)\n        labels = labels.to(DEVICE)\n        \n        logits = model(images)\n        \n        # IMPORTANT: Use softmax probabilities, NOT argmax\n        # This matches training: probs = self.activation_fn(logits)\n        probs = torch.softmax(logits, dim=1)\n        \n        iou_metric.update(probs, labels)\n\n# Compute final IoU\niou_per_class = iou_metric.compute()\n\nprint(f\"\\nIoU per class: {iou_per_class}\")\nprint(f\"\\n=== IoU (seagrass): {iou_per_class[1].item():.4f} ===\")\nprint(f\"=== IoU (background): {iou_per_class[0].item():.4f} ===\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metric - same as training\n",
    "num_classes = model_args[\"num_classes\"]\n",
    "ignore_index = model_args.get(\"ignore_index\", -100)\n",
    "\n",
    "iou_metric = torchmetrics.JaccardIndex(\n",
    "    task=\"multiclass\",\n",
    "    num_classes=num_classes,\n",
    "    ignore_index=ignore_index,\n",
    "    average=\"none\"\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Num classes: {num_classes}, Ignore index: {ignore_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        logits = model(images)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        \n",
    "        iou_metric.update(preds, labels)\n",
    "\n",
    "# Compute final IoU\n",
    "iou_per_class = iou_metric.compute()\n",
    "\n",
    "print(f\"\\nIoU per class: {iou_per_class}\")\n",
    "print(f\"\\n=== IoU (seagrass): {iou_per_class[1].item():.4f} ===\")\n",
    "print(f\"=== IoU (background): {iou_per_class[0].item():.4f} ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}