{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Evaluate segmentation model performance on validation set.\n",
    "Compare models from architecture experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchmetrics\n",
    "import yaml\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.data import NpzSegmentationDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Architecture experiment models\nEXPERIMENT_DIR = Path(\"../configs/seagrass-rgb/architecture-experiment\")\nCKPT_DIR = Path(\"../seagrass-rgb\")\n\n# Model configurations: {display_name: (config_filename, run_id, checkpoint_filename)}\nMODELS = {\n    \"UNet++ 512\": (\"unetpp_resnet34_512.yaml\", \"qjfpb4m8\", \"unetpp_resnet34_512_epoch-199_val-iou-0.7050.ckpt\"),\n    \"UNet++ 1024\": (\"unetpp_resnet34_1024.yaml\", \"mdqn7se0\", \"unetpp_resnet34_1024_epoch-199_val-iou-0.7247.ckpt\"),\n    \"SegFormer 512\": (\"segformer_mitb2_512.yaml\", \"3uav2blr\", \"segformer_mitb2_512_epoch-199_val-iou-0.7425.ckpt\"),\n    \"SegFormer 1024\": (\"segformer_mitb2_1024.yaml\", \"jhf1t0ih\", \"segformer_mitb2_1024_epoch-199_val-iou-0.7909.ckpt\"),\n}\n\n# Inference settings\nBATCH_SIZE = 16\nNUM_WORKERS = 4\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"Device: {DEVICE}\")\nprint(f\"Models to evaluate: {list(MODELS.keys())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": "def load_model(config_path, ckpt_path, device):\n    \"\"\"Load model from config and checkpoint.\"\"\"\n    with open(config_path) as f:\n        config = yaml.safe_load(f)\n    \n    model_config = config[\"model\"]\n    class_path = model_config[\"class_path\"]\n    init_args = model_config[\"init_args\"]\n    \n    module_name, class_name = class_path.rsplit(\".\", 1)\n    module = importlib.import_module(module_name)\n    model_class = getattr(module, class_name)\n    \n    model = model_class(**init_args)\n    checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    model.eval()\n    model.to(device)\n    \n    return model, config\n\n\ndef create_metrics(num_classes, ignore_index, device):\n    \"\"\"Create torchmetrics matching training setup.\"\"\"\n    task = \"binary\" if num_classes == 1 else \"multiclass\"\n    \n    return {\n        \"IoU\": torchmetrics.JaccardIndex(\n            task=task, num_classes=num_classes, ignore_index=ignore_index, average=\"none\"\n        ).to(device),\n        \"Precision\": torchmetrics.Precision(\n            task=task, num_classes=num_classes, ignore_index=ignore_index, average=\"none\"\n        ).to(device),\n        \"Recall\": torchmetrics.Recall(\n            task=task, num_classes=num_classes, ignore_index=ignore_index, average=\"none\"\n        ).to(device),\n    }\n\n\n@torch.no_grad()\ndef evaluate_model(model, dataloader, num_classes, ignore_index, device, target_class=1):\n    \"\"\"Evaluate model and return metrics for target class.\"\"\"\n    model.eval()\n    metrics = create_metrics(num_classes, ignore_index, device)\n    \n    for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        logits = model(images)\n        \n        if logits.shape[1] == 1:\n            preds = (torch.sigmoid(logits).squeeze(1) > 0.5).long()\n        else:\n            preds = logits.argmax(dim=1)\n        \n        for m in metrics.values():\n            m.update(preds, labels)\n    \n    # Extract target class metrics\n    results = {}\n    for name, m in metrics.items():\n        value = m.compute()\n        results[name] = value[target_class].item() if len(value) > 1 else value.item()\n    \n    return results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "results = {}\n\nfor model_name, (config_file, run_id, ckpt_file) in MODELS.items():\n    print(f\"\\nEvaluating: {model_name}\")\n    \n    config_path = EXPERIMENT_DIR / config_file\n    ckpt_path = CKPT_DIR / run_id / \"checkpoints\" / ckpt_file\n    \n    if not ckpt_path.exists():\n        print(f\"  Checkpoint not found: {ckpt_path}\")\n        continue\n    \n    # Load model and config\n    model, config = load_model(config_path, ckpt_path, DEVICE)\n    data_config = config[\"data\"][\"init_args\"]\n    model_config = config[\"model\"][\"init_args\"]\n    \n    num_classes = model_config[\"num_classes\"]\n    ignore_index = model_config.get(\"ignore_index\", -100)\n    \n    # Load validation dataset\n    test_transforms = A.from_dict(data_config[\"test_transforms\"])\n    val_dataset = NpzSegmentationDataset(data_config[\"val_chip_dir\"], transforms=test_transforms)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n    \n    print(f\"  Validation tiles: {len(val_dataset)}\")\n    \n    images, labels = next(iter(val_loader))\n    with torch.no_grad():\n      logits = model(images.to(DEVICE))\n      preds = logits.argmax(dim=1)\n    print(f\"Unique preds: {preds.unique()}\")\n    print(f\"Unique labels: {labels.unique()}\")\n    print(f\"Pred seagrass %: {(preds == 1).float().mean():.3f}\")\n    print(f\"Label seagrass %: {(labels == 1).float().mean():.3f}\")\n    \n    print(test_transforms)\n    # Evaluate\n    results[model_name] = evaluate_model(model, val_loader, num_classes, ignore_index, DEVICE)\n    \n    # Clean up\n    del model\n    torch.cuda.empty_cache()\n    \n    print(f\"  IoU: {results[model_name]['IoU']:.4f}\")\n\nprint(f\"\\nEvaluated {len(results)} models\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Table\n",
    "\n",
    "Models in columns, metrics in rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Metrics (Seagrass Class)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SegFormer 1024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IoU</th>\n",
       "      <td>0.6252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.8597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.6963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.7694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           SegFormer 1024\n",
       "IoU                0.6252\n",
       "Precision          0.8597\n",
       "Recall             0.6963\n",
       "F1                 0.7694"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create table: metrics as rows, models as columns\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_table = df_results.T.round(4)\n",
    "\n",
    "print(\"Validation Set Metrics (Seagrass Class)\")\n",
    "print(\"=\" * 60)\n",
    "display(df_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to outputs/architecture_experiment_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Export table for paper\n",
    "df_table.to_csv(\"../outputs/architecture_experiment_metrics.csv\")\n",
    "print(\"Saved to outputs/architecture_experiment_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Plot\n",
    "\n",
    "Models on x-axis, faceted by metric. Academic ggplot style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ggplot style\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.size\": 10,\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"xtick.labelsize\": 9,\n",
    "    \"ytick.labelsize\": 9,\n",
    "    \"legend.fontsize\": 9,\n",
    "    \"figure.dpi\": 150,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "})\n",
    "\n",
    "# Color palette\n",
    "COLORS = [\"#4C72B0\", \"#55A868\", \"#C44E52\", \"#8172B3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for plotting\n",
    "df_long = df_results.reset_index().melt(\n",
    "    id_vars=\"index\", \n",
    "    var_name=\"Metric\", \n",
    "    value_name=\"Value\"\n",
    ")\n",
    "df_long = df_long.rename(columns={\"index\": \"Model\"})\n",
    "\n",
    "# Create faceted bar plot\n",
    "metrics = [\"IoU\", \"Precision\", \"Recall\", \"F1\"]\n",
    "models = list(results.keys())\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3.5), sharey=True)\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    values = [results[m][metric] for m in models]\n",
    "    bars = ax.bar(range(len(models)), values, color=COLORS, edgecolor=\"white\", linewidth=0.5)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2, \n",
    "            bar.get_height() + 0.01, \n",
    "            f\"{val:.3f}\", \n",
    "            ha=\"center\", \n",
    "            va=\"bottom\", \n",
    "            fontsize=8\n",
    "        )\n",
    "    \n",
    "    ax.set_title(metric, fontweight=\"bold\")\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels([m.replace(\" \", \"\\n\") for m in models], rotation=0)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "fig.suptitle(\"Architecture Experiment: Validation Metrics\", fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/architecture_experiment_barplot.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.savefig(\"../outputs/architecture_experiment_barplot.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved to outputs/architecture_experiment_barplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find best model for each metric\nprint(\"Best model by metric:\")\nprint(\"-\" * 40)\nfor metric in [\"IoU\", \"Precision\", \"Recall\"]:\n    best_model = max(results, key=lambda m: results[m][metric])\n    best_value = results[best_model][metric]\n    print(f\"{metric:12s}: {best_model} ({best_value:.4f})\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hakai-ml-train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
