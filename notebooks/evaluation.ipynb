{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Evaluate segmentation model performance on validation set.\n",
    "Compare models from architecture experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import importlib\nimport sys\nfrom pathlib import Path\n\n# Add project root to path\nsys.path.insert(0, str(Path.cwd().parent))\n\nimport albumentations as A\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torchmetrics\nimport yaml\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\nfrom src.data import NpzSegmentationDataset"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Architecture experiment models\nEXPERIMENT_DIR = Path(\"../configs/seagrass-rgb/architecture-experiment\")\nCKPT_DIR = Path(\"../checkpoints/seagrass-rgb\")\n\n# Model configurations: {display_name: (config_filename, run_id)}\n# UPDATE run_ids to match your W&B runs\nMODELS = {\n    \"UNet++ 512\": (\"unetpp_resnet34_512.yaml\", \"RUNID_HERE\"),\n    \"UNet++ 1024\": (\"unetpp_resnet34_1024.yaml\", \"RUNID_HERE\"),\n    \"SegFormer 512\": (\"segformer_mitb2_512.yaml\", \"RUNID_HERE\"),\n    \"SegFormer 1024\": (\"segformer_mitb2_1024.yaml\", \"RUNID_HERE\"),\n}\n\n# Inference settings\nBATCH_SIZE = 16\nNUM_WORKERS = 4\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"Device: {DEVICE}\")\nprint(f\"Models to evaluate: {list(MODELS.keys())}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config_path, ckpt_path, device):\n",
    "    \"\"\"Load model from config and checkpoint.\"\"\"\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    model_config = config[\"model\"]\n",
    "    class_path = model_config[\"class_path\"]\n",
    "    init_args = model_config[\"init_args\"]\n",
    "    \n",
    "    module_name, class_name = class_path.rsplit(\".\", 1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    model_class = getattr(module, class_name)\n",
    "    \n",
    "    model = model_class(**init_args)\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    return model, config\n",
    "\n",
    "\n",
    "def create_metrics(num_classes, ignore_index, device):\n",
    "    \"\"\"Create torchmetrics matching training setup.\"\"\"\n",
    "    task = \"binary\" if num_classes == 1 else \"multiclass\"\n",
    "    \n",
    "    return {\n",
    "        \"IoU\": torchmetrics.JaccardIndex(\n",
    "            task=task, num_classes=num_classes, ignore_index=ignore_index, average=\"none\"\n",
    "        ).to(device),\n",
    "        \"Precision\": torchmetrics.Precision(\n",
    "            task=task, num_classes=num_classes, ignore_index=ignore_index, average=\"none\"\n",
    "        ).to(device),\n",
    "        \"Recall\": torchmetrics.Recall(\n",
    "            task=task, num_classes=num_classes, ignore_index=ignore_index, average=\"none\"\n",
    "        ).to(device),\n",
    "        \"F1\": torchmetrics.F1Score(\n",
    "            task=task, num_classes=num_classes, ignore_index=ignore_index, average=\"none\"\n",
    "        ).to(device),\n",
    "    }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, num_classes, ignore_index, device, target_class=1):\n",
    "    \"\"\"Evaluate model and return metrics for target class.\"\"\"\n",
    "    model.eval()\n",
    "    metrics = create_metrics(num_classes, ignore_index, device)\n",
    "    \n",
    "    for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits = model(images)\n",
    "        \n",
    "        if logits.shape[1] == 1:\n",
    "            preds = (torch.sigmoid(logits).squeeze(1) > 0.5).long()\n",
    "        else:\n",
    "            preds = logits.argmax(dim=1)\n",
    "        \n",
    "        for m in metrics.values():\n",
    "            m.update(preds, labels)\n",
    "    \n",
    "    # Extract target class metrics\n",
    "    results = {}\n",
    "    for name, m in metrics.items():\n",
    "        value = m.compute()\n",
    "        results[name] = value[target_class].item() if len(value) > 1 else value.item()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for model_name, (config_file, ckpt_folder) in MODELS.items():\n",
    "    print(f\"\\nEvaluating: {model_name}\")\n",
    "    \n",
    "    config_path = EXPERIMENT_DIR / config_file\n",
    "    ckpt_path = CKPT_DIR / ckpt_folder / \"last.ckpt\"\n",
    "    \n",
    "    if not ckpt_path.exists():\n",
    "        print(f\"  Checkpoint not found: {ckpt_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Load model and config\n",
    "    model, config = load_model(config_path, ckpt_path, DEVICE)\n",
    "    data_config = config[\"data\"][\"init_args\"]\n",
    "    model_config = config[\"model\"][\"init_args\"]\n",
    "    \n",
    "    num_classes = model_config[\"num_classes\"]\n",
    "    ignore_index = model_config.get(\"ignore_index\", -100)\n",
    "    \n",
    "    # Load validation dataset\n",
    "    test_transforms = A.from_dict(data_config[\"test_transforms\"])\n",
    "    val_dataset = NpzSegmentationDataset(data_config[\"val_chip_dir\"], transforms=test_transforms)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "    \n",
    "    print(f\"  Validation tiles: {len(val_dataset)}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    results[model_name] = evaluate_model(model, val_loader, num_classes, ignore_index, DEVICE)\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"  IoU: {results[model_name]['IoU']:.4f}\")\n",
    "\n",
    "print(f\"\\nEvaluated {len(results)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Table\n",
    "\n",
    "Models in columns, metrics in rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table: metrics as rows, models as columns\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_table = df_results.T.round(4)\n",
    "\n",
    "print(\"Validation Set Metrics (Seagrass Class)\")\n",
    "print(\"=\" * 60)\n",
    "display(df_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export table for paper\n",
    "df_table.to_csv(\"../outputs/architecture_experiment_metrics.csv\")\n",
    "print(\"Saved to outputs/architecture_experiment_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Plot\n",
    "\n",
    "Models on x-axis, faceted by metric. Academic ggplot style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ggplot style\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.size\": 10,\n",
    "    \"axes.labelsize\": 11,\n",
    "    \"axes.titlesize\": 12,\n",
    "    \"xtick.labelsize\": 9,\n",
    "    \"ytick.labelsize\": 9,\n",
    "    \"legend.fontsize\": 9,\n",
    "    \"figure.dpi\": 150,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "})\n",
    "\n",
    "# Color palette\n",
    "COLORS = [\"#4C72B0\", \"#55A868\", \"#C44E52\", \"#8172B3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for plotting\n",
    "df_long = df_results.reset_index().melt(\n",
    "    id_vars=\"index\", \n",
    "    var_name=\"Metric\", \n",
    "    value_name=\"Value\"\n",
    ")\n",
    "df_long = df_long.rename(columns={\"index\": \"Model\"})\n",
    "\n",
    "# Create faceted bar plot\n",
    "metrics = [\"IoU\", \"Precision\", \"Recall\", \"F1\"]\n",
    "models = list(results.keys())\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3.5), sharey=True)\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    values = [results[m][metric] for m in models]\n",
    "    bars = ax.bar(range(len(models)), values, color=COLORS, edgecolor=\"white\", linewidth=0.5)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2, \n",
    "            bar.get_height() + 0.01, \n",
    "            f\"{val:.3f}\", \n",
    "            ha=\"center\", \n",
    "            va=\"bottom\", \n",
    "            fontsize=8\n",
    "        )\n",
    "    \n",
    "    ax.set_title(metric, fontweight=\"bold\")\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels([m.replace(\" \", \"\\n\") for m in models], rotation=0)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "fig.suptitle(\"Architecture Experiment: Validation Metrics\", fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/architecture_experiment_barplot.png\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.savefig(\"../outputs/architecture_experiment_barplot.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved to outputs/architecture_experiment_barplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model for each metric\n",
    "print(\"Best model by metric:\")\n",
    "print(\"-\" * 40)\n",
    "for metric in metrics:\n",
    "    best_model = max(results, key=lambda m: results[m][metric])\n",
    "    best_value = results[best_model][metric]\n",
    "    print(f\"{metric:12s}: {best_model} ({best_value:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}