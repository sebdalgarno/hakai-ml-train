{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Evaluate segmentation model performance on validation and test sets.\n",
    "\n",
    "Uses torchmetrics for metrics calculation (matches W&B/training metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchmetrics\n",
    "import yaml\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.data import NpzSegmentationDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths - UPDATE THESE\n",
    "CONFIG_PATH = Path(\"../configs/seagrass-rgb/unetpp_resnet34_minimal_aug.yaml\")\n",
    "CKPT_PATH = Path(\"../checkpoints/unetpp_resnet34_minimal_aug/last.ckpt\")  # or download from W&B\n",
    "\n",
    "# Inference settings\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open(CONFIG_PATH) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Extract model and data settings\n",
    "model_config = config[\"model\"]\n",
    "data_config = config[\"data\"][\"init_args\"]\n",
    "\n",
    "num_classes = model_config[\"init_args\"][\"num_classes\"]\n",
    "ignore_index = model_config[\"init_args\"].get(\"ignore_index\", -100)\n",
    "class_names = model_config[\"init_args\"].get(\"class_names\", [\"bg\", \"target\"])\n",
    "\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Ignore index: {ignore_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "class_path = model_config[\"class_path\"]\n",
    "init_args = model_config[\"init_args\"]\n",
    "\n",
    "module_name, class_name = class_path.rsplit(\".\", 1)\n",
    "module = importlib.import_module(module_name)\n",
    "model_class = getattr(module, class_name)\n",
    "\n",
    "model = model_class(**init_args)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(CKPT_PATH, map_location=\"cpu\", weights_only=False)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "\n",
    "print(f\"Model loaded from {CKPT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transforms from config\n",
    "test_transforms = A.from_dict(data_config[\"test_transforms\"])\n",
    "\n",
    "# Dataset that also returns filename\n",
    "class EvalDataset(NpzSegmentationDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        chip_path = self.chips[idx]\n",
    "        data = np.load(chip_path)\n",
    "        image, label = data[\"image\"], data[\"label\"]\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            augmented = self.transforms(image=image, mask=label)\n",
    "            image, label = augmented[\"image\"], augmented[\"mask\"]\n",
    "        \n",
    "        return image, label, chip_path.name\n",
    "\n",
    "# Load datasets\n",
    "val_dataset = EvalDataset(data_config[\"val_chip_dir\"], transforms=test_transforms)\n",
    "test_dataset = EvalDataset(data_config[\"test_chip_dir\"], transforms=test_transforms)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n",
    "\n",
    "print(f\"Validation: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Metrics\n",
    "\n",
    "Using torchmetrics to match training metrics exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics(num_classes, ignore_index, device):\n",
    "    \"\"\"Create torchmetrics matching training setup.\"\"\"\n",
    "    task = \"binary\" if num_classes == 1 else \"multiclass\"\n",
    "    \n",
    "    return {\n",
    "        \"iou\": torchmetrics.JaccardIndex(\n",
    "            task=task, num_classes=num_classes, ignore_index=ignore_index, average=\"none\"\n",
    "        ).to(device),\n",
    "        \"precision\": torchmetrics.Precision(\n",
    "            task=task, num_classes=num_classes, ignore_index=ignore_index, average=\"none\"\n",
    "        ).to(device),\n",
    "        \"recall\": torchmetrics.Recall(\n",
    "            task=task, num_classes=num_classes, ignore_index=ignore_index, average=\"none\"\n",
    "        ).to(device),\n",
    "        \"f1\": torchmetrics.F1Score(\n",
    "            task=task, num_classes=num_classes, ignore_index=ignore_index, average=\"none\"\n",
    "        ).to(device),\n",
    "    }\n",
    "\n",
    "def reset_metrics(metrics):\n",
    "    for m in metrics.values():\n",
    "        m.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ortho(filename):\n",
    "    \"\"\"Extract ortho name from chip filename (e.g., 'calmus_u0421_x123_y456.npz' -> 'calmus_u0421').\"\"\"\n",
    "    match = re.match(r\"^(.+?)_x\\d+_y\\d+\\.npz$\", filename)\n",
    "    return match.group(1) if match else filename.replace(\".npz\", \"\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, metrics, device, num_classes):\n",
    "    \"\"\"Run evaluation and collect per-sample results.\"\"\"\n",
    "    model.eval()\n",
    "    reset_metrics(metrics)\n",
    "    \n",
    "    # For per-sample metrics\n",
    "    per_sample_iou = torchmetrics.JaccardIndex(\n",
    "        task=\"multiclass\", num_classes=num_classes, ignore_index=ignore_index, average=\"none\"\n",
    "    ).to(device)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for images, labels, filenames in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(images)\n",
    "        \n",
    "        # Get predictions\n",
    "        if logits.shape[1] == 1:\n",
    "            probs = torch.sigmoid(logits).squeeze(1)\n",
    "            preds = (probs > 0.5).long()\n",
    "        else:\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = probs.argmax(dim=1)\n",
    "        \n",
    "        # Update global metrics\n",
    "        for m in metrics.values():\n",
    "            m.update(preds, labels)\n",
    "        \n",
    "        # Compute per-sample IoU for ortho breakdown\n",
    "        for i, filename in enumerate(filenames):\n",
    "            per_sample_iou.reset()\n",
    "            per_sample_iou.update(preds[i:i+1], labels[i:i+1])\n",
    "            iou_per_class = per_sample_iou.compute()\n",
    "            \n",
    "            results.append({\n",
    "                \"filename\": filename,\n",
    "                \"ortho\": extract_ortho(filename),\n",
    "                \"iou_target\": iou_per_class[1].item() if len(iou_per_class) > 1 else iou_per_class[0].item(),\n",
    "            })\n",
    "    \n",
    "    # Compute final metrics\n",
    "    final_metrics = {name: m.compute() for name, m in metrics.items()}\n",
    "    \n",
    "    return final_metrics, pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics = create_metrics(num_classes, ignore_index, DEVICE)\n",
    "val_results, val_df = evaluate(model, val_loader, val_metrics, DEVICE, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"VALIDATION SET METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nPer-class metrics:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    IoU:       {val_results['iou'][i]:.4f}\")\n",
    "    print(f\"    Precision: {val_results['precision'][i]:.4f}\")\n",
    "    print(f\"    Recall:    {val_results['recall'][i]:.4f}\")\n",
    "    print(f\"    F1:        {val_results['f1'][i]:.4f}\")\n",
    "\n",
    "# Mean across non-background classes (matches W&B val/iou_epoch)\n",
    "mean_iou = val_results['iou'][1:].mean().item()\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Mean IoU (non-bg): {mean_iou:.4f}  <- Should match W&B val/iou_epoch\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = create_metrics(num_classes, ignore_index, DEVICE)\n",
    "test_results, test_df = evaluate(model, test_loader, test_metrics, DEVICE, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"TEST SET METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nPer-class metrics:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    IoU:       {test_results['iou'][i]:.4f}\")\n",
    "    print(f\"    Precision: {test_results['precision'][i]:.4f}\")\n",
    "    print(f\"    Recall:    {test_results['recall'][i]:.4f}\")\n",
    "    print(f\"    F1:        {test_results['f1'][i]:.4f}\")\n",
    "\n",
    "mean_iou = test_results['iou'][1:].mean().item()\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Mean IoU (non-bg): {mean_iou:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Ortho Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by ortho\n",
    "def ortho_summary(df):\n",
    "    return df.groupby(\"ortho\").agg(\n",
    "        iou_mean=(\"iou_target\", \"mean\"),\n",
    "        iou_std=(\"iou_target\", \"std\"),\n",
    "        iou_min=(\"iou_target\", \"min\"),\n",
    "        iou_max=(\"iou_target\", \"max\"),\n",
    "        n_tiles=(\"iou_target\", \"count\"),\n",
    "    ).sort_values(\"iou_mean\", ascending=False).round(4)\n",
    "\n",
    "print(\"VALIDATION - Per-Ortho IoU (target class)\")\n",
    "print(\"=\" * 70)\n",
    "display(ortho_summary(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST - Per-Ortho IoU (target class)\")\n",
    "print(\"=\" * 70)\n",
    "display(ortho_summary(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "target_idx = 1  # seagrass class\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Metric\": [\"IoU\", \"Precision\", \"Recall\", \"F1\"],\n",
    "    \"Validation\": [\n",
    "        val_results[\"iou\"][target_idx].item(),\n",
    "        val_results[\"precision\"][target_idx].item(),\n",
    "        val_results[\"recall\"][target_idx].item(),\n",
    "        val_results[\"f1\"][target_idx].item(),\n",
    "    ],\n",
    "    \"Test\": [\n",
    "        test_results[\"iou\"][target_idx].item(),\n",
    "        test_results[\"precision\"][target_idx].item(),\n",
    "        test_results[\"recall\"][target_idx].item(),\n",
    "        test_results[\"f1\"][target_idx].item(),\n",
    "    ],\n",
    "}).round(4)\n",
    "\n",
    "print(f\"\\nSUMMARY - {class_names[target_idx]} class metrics\")\n",
    "print(\"=\" * 50)\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Per-Ortho Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, (name, df) in zip(axes, [(\"Validation\", val_df), (\"Test\", test_df)]):\n",
    "    ortho_stats = ortho_summary(df)\n",
    "    \n",
    "    x = range(len(ortho_stats))\n",
    "    ax.bar(x, ortho_stats[\"iou_mean\"], yerr=ortho_stats[\"iou_std\"], capsize=3, color=\"steelblue\", alpha=0.8)\n",
    "    ax.axhline(ortho_stats[\"iou_mean\"].mean(), color=\"red\", linestyle=\"--\", label=f\"Mean: {ortho_stats['iou_mean'].mean():.3f}\")\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(ortho_stats.index, rotation=45, ha=\"right\", fontsize=8)\n",
    "    ax.set_ylabel(\"IoU\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(f\"{name} - IoU by Ortho\")\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
